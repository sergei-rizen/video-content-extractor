name: Process Videos from Dropbox

on:
  schedule:
    - cron: '*/30 * * * *'  # Run every 30 minutes
  workflow_dispatch:  # Allow manual triggering

jobs:
  check-and-process:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        # This action downloads your repository code, including fixed-python-script.py
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        # Update dependencies to include 'dropbox' and remove Google Drive specific ones
        run: |
          python -m pip install --upgrade pip
          pip install dropbox google.generativeai markdown

      # --- State Persistence: Download previously processed files list ---
      # This downloads the artifact from the *most recent successful* workflow run
      # if it exists. The processed_files.json file is needed by the script
      # to know which files have already been handled.
      - name: Download processed files state
        uses: actions/download-artifact@v3
        with:
          name: processed-files-state # Name of the artifact to download
          path: .                     # Download it to the current directory

      # --- Remove Google Drive credential step ---
      # This is no longer needed as the script uses Dropbox token directly
      # - name: Create service account key file
      #   run: |
      #     echo '${{ secrets.GOOGLE_CREDENTIALS }}' > service-account.json

      # --- Remove script creation step ---
      # The script is now an external file
      # - name: Create combined processing script
      #   run: |
      #     cat > process_and_upload.py << 'EOF'
      #     ... (Python code) ...
      #     EOF

      - name: Run the processing script
        # Execute the external Python script file
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          DROPBOX_ACCESS_TOKEN: ${{ secrets.DROPBOX_ACCESS_TOKEN }} # Pass the Dropbox secret
        run: python fixed-python-script.py # <-- Execute the external file

      # --- State Persistence: Upload updated processed files list ---
      # This uploads the processed_files.json file generated by the script
      # as an artifact, making it available for the next workflow run.
      # Always run this step, even if the script had errors, to save partial state
      - name: Upload processed files state
        uses: actions/upload-artifact@v3
        if: always() # Upload artifact even if previous steps failed
        with:
          name: processed-files-state # Name the artifact
          path: processed_files.json  # Path to the file to upload
          retention-days: 7         # Optional: how long to keep the artifact (e.g., 7 days)
